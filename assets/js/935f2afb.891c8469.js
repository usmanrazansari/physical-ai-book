"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8581],{5610(i){i.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction to Physical AI & Humanoid Robotics","href":"/physical-ai-book/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"ROS 2 Architecture","href":"/physical-ai-book/docs/module-1-ros2/ch1-ros2-architecture","docId":"module-1-ros2/ch1-ros2-architecture","unlisted":false},{"type":"link","label":"Python Agents and URDF for Humanoids","href":"/physical-ai-book/docs/module-1-ros2/ch2-python-agents-urdf","docId":"module-1-ros2/ch2-python-agents-urdf","unlisted":false}],"href":"/physical-ai-book/docs/module-1-ros2"},{"type":"category","label":"Module 2: The Digital Twin","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Physics Simulation in Gazebo","href":"/physical-ai-book/docs/module-2-digital-twin/ch1-gazebo-physics-simulation","docId":"module-2-digital-twin/ch1-gazebo-physics-simulation","unlisted":false},{"type":"link","label":"Unity Digital Twins and Sensor Simulation","href":"/physical-ai-book/docs/module-2-digital-twin/ch2-unity-digital-twins","docId":"module-2-digital-twin/ch2-unity-digital-twins","unlisted":false}],"href":"/physical-ai-book/docs/module-2-digital-twin"},{"type":"category","label":"Module 3: The AI-Robot Brain","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Isaac Sim and Synthetic Data","href":"/physical-ai-book/docs/module-3-ai-robot-brain/ch1-isaac-sim-synthetic-data","docId":"module-3-ai-robot-brain/ch1-isaac-sim-synthetic-data","unlisted":false},{"type":"link","label":"Isaac ROS, VSLAM, and Nav2 Navigation","href":"/physical-ai-book/docs/module-3-ai-robot-brain/ch2-isaac-ros-vslam-nav2","docId":"module-3-ai-robot-brain/ch2-isaac-ros-vslam-nav2","unlisted":false}],"href":"/physical-ai-book/docs/module-3-ai-robot-brain"},{"type":"category","label":"Module 4: Vision-Language-Action","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Voice-to-Action and LLM Planning","href":"/physical-ai-book/docs/module-4-vla/ch1-voice-to-action-llm-planning","docId":"module-4-vla/ch1-voice-to-action-llm-planning","unlisted":false},{"type":"link","label":"Capstone: Autonomous Humanoid System","href":"/physical-ai-book/docs/module-4-vla/ch2-capstone-autonomous-humanoid","docId":"module-4-vla/ch2-capstone-autonomous-humanoid","unlisted":false}],"href":"/physical-ai-book/docs/module-4-vla"}]},"docs":{"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the comprehensive educational resource on Physical AI & Humanoid Robotics. This book explores the fascinating intersection of artificial intelligence and physical systems, focusing on embodied intelligence and AI systems operating in the physical world.","sidebar":"tutorialSidebar"},"module-1-ros2/ch1-ros2-architecture":{"id":"module-1-ros2/ch1-ros2-architecture","title":"ROS 2 Architecture: Nodes, Topics, and Services","description":"Understanding the fundamental communication patterns in Robot Operating System 2: nodes, topics, and services for robotic applications","sidebar":"tutorialSidebar"},"module-1-ros2/ch2-python-agents-urdf":{"id":"module-1-ros2/ch2-python-agents-urdf","title":"Python Agents (rclpy) and URDF for Humanoids","description":"Understanding Python-based ROS 2 clients with rclpy and Unified Robot Description Format for humanoid robot modeling","sidebar":"tutorialSidebar"},"module-1-ros2/index":{"id":"module-1-ros2/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Foundational concepts of Robot Operating System 2 (ROS 2) architecture, nodes, topics, services, and humanoid robot modeling with URDF","sidebar":"tutorialSidebar"},"module-2-digital-twin/ch1-gazebo-physics-simulation":{"id":"module-2-digital-twin/ch1-gazebo-physics-simulation","title":"Physics Simulation in Gazebo: Gravity and Collisions","description":"Understanding physics simulation in Gazebo with focus on gravity effects and collision detection for robotic applications","sidebar":"tutorialSidebar"},"module-2-digital-twin/ch2-unity-digital-twins":{"id":"module-2-digital-twin/ch2-unity-digital-twins","title":"Unity Digital Twins and Sensor Simulation","description":"Exploring Unity as a platform for digital twins with focus on LiDAR, depth, and IMU sensor simulation for robotic applications","sidebar":"tutorialSidebar"},"module-2-digital-twin/index":{"id":"module-2-digital-twin/index","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"Physics simulation in Gazebo with gravity and collisions, and Unity digital twins with sensor simulation for LiDAR, depth, and IMU","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/ch1-isaac-sim-synthetic-data":{"id":"module-3-ai-robot-brain/ch1-isaac-sim-synthetic-data","title":"Isaac Sim and Synthetic Data Generation","description":"Understanding NVIDIA Isaac Sim for generating synthetic data to train AI models for robotic applications","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/ch2-isaac-ros-vslam-nav2":{"id":"module-3-ai-robot-brain/ch2-isaac-ros-vslam-nav2","title":"Isaac ROS, VSLAM, and Nav2 Navigation","description":"Understanding NVIDIA Isaac ROS integration, Visual SLAM, and Nav2 navigation systems for autonomous robotic movement","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/index":{"id":"module-3-ai-robot-brain/index","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","description":"NVIDIA Isaac Sim for synthetic data generation and Isaac ROS integration with VSLAM and Nav2 navigation systems","sidebar":"tutorialSidebar"},"module-4-vla/ch1-voice-to-action-llm-planning":{"id":"module-4-vla/ch1-voice-to-action-llm-planning","title":"Voice-to-Action and LLM Planning","description":"Understanding how voice commands are converted to robotic actions through Large Language Model planning in Vision-Language-Action systems","sidebar":"tutorialSidebar"},"module-4-vla/ch2-capstone-autonomous-humanoid":{"id":"module-4-vla/ch2-capstone-autonomous-humanoid","title":"Capstone: Autonomous Humanoid System Integration","description":"Integration of all previous concepts into a complete autonomous humanoid system demonstrating the full application of physical AI concepts","sidebar":"tutorialSidebar"},"module-4-vla/index":{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Voice-to-action systems with LLM planning and the capstone autonomous humanoid system integrating all previous concepts","sidebar":"tutorialSidebar"}}}')}}]);